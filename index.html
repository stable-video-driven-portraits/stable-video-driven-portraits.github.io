<!DOCTYPE html>
<html lang="en" class="bg-gray-50 text-gray-900">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Stable Video-Driven Portraits</title>
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body class="max-w-4xl mx-auto p-6">
    <!-- Header -->


    <header class="mb-10 text-center">
      <h1 class="text-4xl font-bold mb-2">Stable Video-Driven Portraits</h1>
      
      <p class="text-lg text-gray-700">
        <a href="https://people.mpi-inf.mpg.de/~mbr/" class="text-blue-600 hover:underline">Mallikarjun B R</a><sup>1</sup>,
        <a href="https://feiiyin.github.io/" class="text-blue-600 hover:underline">Fei Yin<sup>1,2</sup>,
        <a href="https://voletiv.github.io/" class="text-blue-600 hover:underline">Vikram Voleti<sup>1</sup>,
        <a href="https://neeek2303.github.io/" class="text-blue-600 hover:underline">Nikita Drobyshev<sup>1,3</sup>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/alumni-and-former-members/maksim-lapin/" class="text-blue-600 hover:underline">Maksim Lapin<sup>1</sup>,
        <a href="https://www.aaryaman.net/" class="text-blue-600 hover:underline">Aaryaman Vasishta<sup>1</sup>,
        <a href="https://varunjampani.github.io/" class="text-blue-600 hover:underline">Varun Jampani<sup>1</sup>
      </p>
    
      <p class="text-sm text-gray-500 mt-1">
        <sup>1</sup>Stability AI &nbsp; 
        <sup>2</sup>University of Cambridge 3 &nbsp; 
        <sup>3</sup>Cantina
      </p>
    
      <a href="paper.pdf" target="_blank" class="text-blue-600 underline mt-3 inline-block">Download Paper (PDF)</a>
    </header>


    

 <!-- Abstract -->
 <section class="mb-10">
  <h2 class="text-2xl font-semibold mb-3"> Teaser </h2>
  <p class="text-gray-700 leading-relaxed">
        <img src="teaser.png" alt="Trulli" width="1000" > <!-- height="333"> -->
      </p>
    </section>

    <!-- Abstract -->

    <!-- Abstract -->
    <section class="mb-10">
      <h2 class="text-2xl font-semibold mb-3">Abstract</h2>
      <p class="text-gray-700 leading-relaxed">
       
Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. 

In this work, we propose a novel diffusion-based framework that leverages masked facial regions—specifically the eyes, nose, and mouth—from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross-identity supervision. 

To leverage the strong prior from the pre-trained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization.
We introduce spatial-temporal attention mechanisms that allow inter-frame and intra-frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. 

At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. 

Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.
 
      </p>
    </section>
 <!-- Abstract -->
 <section class="mb-10">
  <h2 class="text-2xl font-semibold mb-3"> Overview </h2>

  <div>
    <video controls class="rounded-lg w-full shadow-md">
      <source src="svdp_ov_output.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>

  </section>


    <!-- Video Results -->
    <section>
      <h2 class="text-2xl font-semibold mb-6">Video Results</h2>
      <div class="grid grid-cols-3 md:grid-cols-1 gap-6">
        <!-- Video 1 -->
        <div>
          <video controls class="rounded-lg w-full shadow-md">
            <source src="self.mp4" type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          <p class="mt-2 text-sm text-gray-600">Self-Reenactment</p>
        </div>
        <!-- Video 2 -->
        <div>
          <video controls class="rounded-lg w-full shadow-md">
            <source src="cross1.mp4" type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          <p class="mt-2 text-sm text-gray-600">Cross-Reenactment: On real humans</p>
        </div>
        <!-- Video 2 -->
        <div>
          <video controls class="rounded-lg w-full shadow-md">
            <source src="cross2.mp4" type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          <p class="mt-2 text-sm text-gray-600">Cross-Reenactment: On different styles</p>
        </div>
        <!-- Add more videos as needed -->
      </div>
    </section>

<pre>
@article{svdp,
  title={Stable Video-Driven Portraits},
  author={B. R., Mallikarjun and Yin, Fei and Voleti, Vikram and Drobyshev, Nikita and Lapin, Maksim and Vasishta, Aaryaman and Jampani, Varun},
  title={Stable Video-Driven Portraits}, 
  author={Mallikarjun B. R. and Fei Yin and Vikram Voleti and Nikita Drobyshev and Maksim Lapin and Aaryaman Vasishta and Varun Jampani},
  year={2025},
  eprint={2509.17476},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2509.17476}, 
}
</pre>

          
    <!-- Footer -->
    <footer class="mt-16 text-center text-gray-500 text-sm">
      © 2025 StabilityAI 
    </footer>
  </body>
</html>

